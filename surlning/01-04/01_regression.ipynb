{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0ecbae",
   "metadata": {},
   "source": [
    "### **Lesson 1 ‚Äì Supervised Learning (Regression c∆° b·∫£n)**\n",
    "\n",
    "**üîë Prerequisite**\n",
    "\n",
    "* Bi·∫øt **Python** (m√†y c√≥ r·ªìi).\n",
    "* Hi·ªÉu **array, h√†m s·ªë, ƒë·ªì th·ªã** c∆° b·∫£n.\n",
    "* Bi·∫øt ch√∫t v·ªÅ **vector/matrix** th√¨ c√†ng t·ªët.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ùì Supervised Learning l√† c√°i qu√°i g√¨?\n",
    "\n",
    "* **Supervised Learning (SL)**: c√≥ **d·ªØ li·ªáu ƒë·∫ßu v√†o (X)** + **nh√£n ƒë√∫ng (y)**.\n",
    "* M·ª•c ti√™u: h·ªçc m·ªôt **h√†m f(X) ‚âà y**.\n",
    "* V√≠ d·ª•:\n",
    "\n",
    "  * Input = di·ªán t√≠ch nh√†, Output = gi√° nh√†.\n",
    "  * Input = ƒëi·ªÉm ki·ªÉm tra, Output = ƒëi·ªÉm t·ªïng k·∫øt.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ùì Regression kh√°c Classification nh∆∞ th·∫ø n√†o?\n",
    "\n",
    "* **Regression** ‚Üí d·ª± ƒëo√°n gi√° tr·ªã **li√™n t·ª•c** (continuous).\n",
    "* **Classification** ‚Üí d·ª± ƒëo√°n nh√£n **r·ªùi r·∫°c** (discrete).\n",
    "* V√≠ d·ª•:\n",
    "\n",
    "  * Regression: d·ª± ƒëo√°n c√¢n n·∫∑ng (50.2kg, 71.5kg).\n",
    "  * Classification: d·ª± ƒëo√°n con m√®o hay con ch√≥.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ùì T·∫°i sao Linear Regression l·∫°i hay ƒë∆∞·ª£c d√πng ƒë·∫ßu ti√™n?\n",
    "\n",
    "* ƒê∆°n gi·∫£n, d·ªÖ hi·ªÉu.\n",
    "* M√¥ h√¨nh:\n",
    "\n",
    "  $$\n",
    "  \\hat{y} = w \\cdot x + b\n",
    "  $$\n",
    "\n",
    "  (ho·∫∑c nhi·ªÅu chi·ªÅu th√¨ $\\hat{y} = Xw + b$)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ùì L√†m sao ƒëo ‚Äúƒë·ªô sai‚Äù c·ªßa m√¥ h√¨nh?\n",
    "\n",
    "* D√πng **Loss function**.\n",
    "* V·ªõi Regression hay d√πng **Mean Squared Error (MSE)**:\n",
    "\n",
    "  $$\n",
    "  L = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ùì L√†m sao t√¨m w, b t·ªët nh·∫•t?\n",
    "\n",
    "* D√πng **Gradient Descent**: l·∫∑p ƒëi l·∫∑p l·∫°i ƒë·ªÉ gi·∫£m loss.\n",
    "* √ù t∆∞·ªüng: t√≠nh gradient c·ªßa loss, c·∫≠p nh·∫≠t w, b theo h∆∞·ªõng l√†m loss nh·ªè h∆°n.\n",
    "\n",
    "---\n",
    "\n",
    "#### üöÄ Practice Exercise\n",
    "\n",
    "T·ª± code Linear Regression si√™u ƒë∆°n gi·∫£n (1 feature)\n",
    "\n",
    "D·ªØ li·ªáu:\n",
    "\n",
    "```\n",
    "X = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "```\n",
    "\n",
    "üëâ R√µ r√†ng quan h·ªá l√† $y = 2x$.\n",
    "\n",
    "Vi·∫øt code Python:\n",
    "\n",
    "1. Kh·ªüi t·∫°o w, b ng·∫´u nhi√™n.\n",
    "2. T√≠nh loss (MSE).\n",
    "3. Update w, b b·∫±ng gradient descent (v√†i v√≤ng l·∫∑p).\n",
    "4. In k·∫øt qu·∫£ ra.\n",
    "\n",
    "G·ª£i √Ω c√¥ng th·ª©c update:\n",
    "\n",
    "* $w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w}$\n",
    "* $b = b - \\alpha \\cdot \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "V·ªõi:\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial w} = -\\frac{2}{n}\\sum x_i (y_i - \\hat{y}_i)$\n",
    "* $\\frac{\\partial L}{\\partial b} = -\\frac{2}{n}\\sum (y_i - \\hat{y}_i)$\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d04328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98422181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "X = np.arange(1, 6)\n",
    "Y = np.arange(2, 11, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236db1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "w = b = 0\n",
    "lr = 0.01\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3d13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def mse(y: np.ndarray):\n",
    "    return ((Y - y) ** 2).mean()\n",
    "\n",
    "\n",
    "def forward():\n",
    "    return w * X + b\n",
    "\n",
    "\n",
    "def deriv_w(y: np.ndarray):\n",
    "    return -2 * ((Y - y) * X).mean()\n",
    "\n",
    "\n",
    "def deriv_b(y: np.ndarray):\n",
    "    return -2 * ((Y - y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5af51f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 76307.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "def train_step():\n",
    "    global w, b\n",
    "    y = forward()\n",
    "    w -= lr * deriv_w(y)\n",
    "    b -= lr * deriv_b(y)\n",
    "\n",
    "\n",
    "for _ in tqdm(range(epochs)):\n",
    "    train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b93f6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Report\n",
    "print(f\"Final loss: {mse(forward()):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
