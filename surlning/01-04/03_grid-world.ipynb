{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2730c42",
   "metadata": {},
   "source": [
    "### **Lesson 3 ‚Äì Gi·ªõi thi·ªáu & Grid World**\n",
    "\n",
    "**üîë Prerequisite**\n",
    "\n",
    "* Bi·∫øt loop + if/else trong Python.\n",
    "* Bi·∫øt random m·ªôt ch√∫t.\n",
    "* Hi·ªÉu kh√°i ni·ªám reward (th∆∞·ªüng/ph·∫°t) c∆° b·∫£n.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ùì Reinforcement Learning (RL) l√† g√¨?\n",
    "\n",
    "* Kh√°c v·ªõi SL (c√≥ nh√£n s·∫µn) hay UL (gom c·ª•m), RL = **h·ªçc qua t∆∞∆°ng t√°c**.\n",
    "* Th√†nh ph·∫ßn ch√≠nh:\n",
    "\n",
    "  * **Agent**: th·∫±ng ra quy·∫øt ƒë·ªãnh.\n",
    "  * **Environment**: th·∫ø gi·ªõi m√† agent t∆∞∆°ng t√°c.\n",
    "  * **Action**: h√†nh ƒë·ªông agent ch·ªçn.\n",
    "  * **State**: t√¨nh tr·∫°ng hi·ªán t·∫°i c·ªßa m√¥i tr∆∞·ªùng.\n",
    "  * **Reward**: feedback cho action.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ùì RL kh√°c SL/UL ·ªü ƒë√¢u?\n",
    "\n",
    "* SL: h·ªçc t·ª´ nh√£n ƒë√∫ng.\n",
    "* UL: h·ªçc t·ª´ d·ªØ li·ªáu ch∆∞a c√≥ nh√£n.\n",
    "* RL: h·ªçc t·ª´ **tr·∫£i nghi·ªám + th∆∞·ªüng/ph·∫°t**, kh√¥ng ai ƒë∆∞a nh√£n ƒë√∫ng c·∫£.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ùì V√≠ d·ª• mini: Grid World\n",
    "\n",
    "* M·ªôt b√†n c·ªù 3x3.\n",
    "* Agent b·∫Øt ƒë·∫ßu ·ªü g√≥c tr√°i tr√™n (0,0).\n",
    "* Goal = g√≥c ph·∫£i d∆∞·ªõi (2,2).\n",
    "* Action = {up, down, left, right}.\n",
    "* Reward:\n",
    "\n",
    "  * -1 m·ªói b∆∞·ªõc.\n",
    "  * +10 khi ƒë·∫øn goal.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ùì Ch√≠nh s√°ch (policy) l√† g√¨?\n",
    "\n",
    "* **Policy** = c√°ch agent ch·ªçn action t·∫°i m·ªói state.\n",
    "* Ban ƒë·∫ßu agent random.\n",
    "* D·∫ßn d·∫ßn h·ªçc ƒë∆∞·ª£c policy ‚Äúƒëi l·ªëi ng·∫Øn nh·∫•t ƒë·∫øn goal‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "#### üöÄ Practice Exercise\n",
    "\n",
    "Vi·∫øt m·ªôt environment Grid World ƒë∆°n gi·∫£n\n",
    "\n",
    "Y√™u c·∫ßu:\n",
    "\n",
    "1. Represent grid 3x3 b·∫±ng numpy array ho·∫∑c tuple (row, col).\n",
    "2. State = v·ªã tr√≠ hi·ªán t·∫°i (row, col).\n",
    "3. H√†m `step(action)` ‚Üí tr·∫£ v·ªÅ `(next_state, reward, done)`.\n",
    "4. H√†nh ƒë·ªông h·ª£p l·ªá: up/down/left/right. N·∫øu ƒëi ra ngo√†i ‚Üí ·ªü nguy√™n, reward = -1.\n",
    "5. N·∫øu ƒë·∫øn goal (2,2) ‚Üí done=True, reward=+10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "682ae6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3522589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.state: tuple[int, int]\n",
    "        self.cum_reward: int\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        self.cum_reward = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action: str):\n",
    "        match action:\n",
    "            case \"L\":\n",
    "                next_state = (self.state[0], max(0, self.state[1] - 1))\n",
    "            case \"R\":\n",
    "                next_state = (self.state[0], min(2, self.state[1] + 1))\n",
    "            case \"U\":\n",
    "                next_state = (max(0, self.state[0] - 1), self.state[1])\n",
    "            case \"D\":\n",
    "                next_state = (min(2, self.state[0] + 1), self.state[1])\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid action: {action}\")\n",
    "\n",
    "        self.state = next_state\n",
    "\n",
    "        if next_state[0] == next_state[1] == 2:\n",
    "            self.cum_reward += 10\n",
    "            return next_state, 10, True\n",
    "        else:\n",
    "            self.cum_reward -= 1\n",
    "            return next_state, -1, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525d0b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs, max_steps = 20, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73fd835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model = GridWorld()\n",
    "\n",
    "# best = max cum_reward and actions seq\n",
    "best: tuple[int, list[str]] = (-1 << 30, [])\n",
    "\n",
    "for _ in range(epochs):\n",
    "    model.reset()\n",
    "\n",
    "    actions, done = [], False\n",
    "    for _ in range(max_steps):\n",
    "        # Assuming policy = random\n",
    "        action = random.choice(\"LRUD\")\n",
    "        actions.append(action)\n",
    "\n",
    "        _, _, done = model.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if done and model.cum_reward > best[0]:\n",
    "        best = (model.cum_reward, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "922ed5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best found solution:\n",
      "* cum_reward: 6\n",
      "* actions seq: LRDRD\n"
     ]
    }
   ],
   "source": [
    "print(\"Best found solution:\")\n",
    "print(f\"* cum_reward: {best[0]}\")\n",
    "print(f\"* actions seq: {''.join(best[1])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
